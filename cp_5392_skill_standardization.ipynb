{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLEAN SKILL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "import pandas as pd\n",
    "import string\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "\n",
    "# BASE_PATH = os.path.dirname(__file__)\n",
    "\n",
    "class CleanSkills():\n",
    "\n",
    "    def __init__(self, input_skill_file,  clean_skill_file, output_skill_file):\n",
    "        if type(input_skill_file) is pandas.core.frame.DataFrame:\n",
    "            self.input_skills_list = input_skill_file\n",
    "        else:\n",
    "            self.input_skills_list = pd.read_csv(input_skill_file)['skill']\n",
    "\n",
    "        if type(clean_skill_file) is pandas.core.frame.DataFrame:\n",
    "            self.clean_skills_list = clean_skill_file\n",
    "        else:\n",
    "            self.clean_skills_list = pd.read_csv(clean_skill_file)['skill']\n",
    "        self.output_clean_skill_file = output_skill_file\n",
    "        self.res = {}        \n",
    "\n",
    "    def clean_skills(self, output_type='DataFrame'):\n",
    "        result_df = pd.DataFrame()\n",
    "        cols = ['skill', 'clean_skill']\n",
    "\n",
    "        # self.input_skills_list = ['oracle database: 11g', 'self confidant', 'uv visible spectroscopy', 'junior software developer', 'angular material design', 'signal processsing']\n",
    "        for skill in self.input_skills_list:\n",
    "            skill = skill.strip()\n",
    "            if skill.lower() in self.res.keys():\n",
    "                skills_stat_df = pd.DataFrame({'skill':[skill], 'clean_skill':[self.res[skill.lower()]]})\n",
    "                result_df = result_df.append(skills_stat_df)\n",
    "                continue\n",
    "            skill_list, clean_skill_list, total_score_list, ratio_list, partial_ratio_list,\\\n",
    "            sort_ratio_list, partial_sort_ratio_list, set_ratio_list, partial_set_ratio_list = [], [], [], [], [], [], [], [], []\n",
    "            for clean_skill in self.clean_skills_list:\n",
    "                clean_skill = clean_skill.lower()\n",
    "                ratio = fuzz.ratio(skill, clean_skill)\n",
    "                partial_ratio = fuzz.partial_ratio(skill, clean_skill)\n",
    "                sort_ratio = fuzz.token_sort_ratio(skill, clean_skill)\n",
    "                partial_sort_ratio = fuzz.partial_token_sort_ratio(skill, clean_skill)\n",
    "                set_ratio = fuzz.token_set_ratio(skill, clean_skill)\n",
    "                partial_set_ratio = fuzz.partial_token_set_ratio(skill, clean_skill)\n",
    "\n",
    "                total_score = (1 * ratio) + (0.8 * set_ratio) + (0.8 * sort_ratio) + (0.5 * partial_ratio) + \\\n",
    "                              (0.3 * partial_set_ratio) + (0.3 * partial_sort_ratio)\n",
    "                \n",
    "                skill_list.append(skill)\n",
    "                clean_skill_list.append(clean_skill)\n",
    "                total_score_list.append(total_score)\n",
    "                ratio_list.append(ratio)\n",
    "                partial_ratio_list.append(partial_ratio)\n",
    "                sort_ratio_list.append(sort_ratio)\n",
    "                partial_sort_ratio_list.append(partial_sort_ratio)\n",
    "                set_ratio_list.append(set_ratio)\n",
    "                partial_set_ratio_list.append(partial_set_ratio)\n",
    "            skills_stat_df = pd.DataFrame({'skill':skill_list, 'clean_skill':clean_skill_list,\\\n",
    "                            'total_score':total_score_list, 'ratio':ratio_list,\\\n",
    "                            'partial_ratio':partial_ratio_list, 'sort_ratio':sort_ratio_list,\\\n",
    "                            'partial_sort_ratio':partial_sort_ratio_list, 'set_ratio':set_ratio_list, 'partial_set_ratio':partial_set_ratio_list})\\\n",
    "                            .sort_values(by=['total_score', 'ratio', 'sort_ratio', 'set_ratio', 'partial_ratio'], ascending=False)[:1]\n",
    "            self.res[skills_stat_df['skill'][0].lower()] = skills_stat_df['clean_skill'][0]\n",
    "            result_df = result_df.append(skills_stat_df[cols])\n",
    "\n",
    "        if output_type != 'DataFrame':\n",
    "            result_df.to_csv(self.output_clean_skill_file)\n",
    "        else:\n",
    "            return result_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LEMMATISE SKILLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import nltk\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# import csv\n",
    "# import sys\n",
    "# import string\n",
    "\n",
    "# class LemmatiseSkills():\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         self.wordnet_lemmatizer = WordNetLemmatizer()\n",
    "#         # nltk.download('wordnet')\n",
    "\n",
    "#     def lemmatise_file(self, in_file, out_file):\n",
    "#         with open(in_file, 'r') as input_file, open(out_file, 'w') as output_file:\n",
    "#             line = input_file.readline()\n",
    "#             while line:\n",
    "#                 flag = 0\n",
    "#                 for char in line:\n",
    "#                     if ord(char) > 127:\n",
    "#                         flag = 1\n",
    "#                         break\n",
    "#                 if flag:\n",
    "#                     output_file.write('\" \"\\n')\n",
    "#                     line = input_file.readline()\n",
    "#                     continue\n",
    "#                 line = line.translate(str.maketrans('-/\"()&', '      ', ''))\n",
    "#                 new_line = ''\n",
    "#                 for word in line.split(' '):\n",
    "#                     word = word.strip().lower()\n",
    "#                     lemma = self.wordnet_lemmatizer.lemmatize(word)\n",
    "#                     new_line += word+' ' if len(word) <= 4 else lemma+' '\n",
    "#                 line = input_file.readline()\n",
    "#                 output_file.write('\"{new_line}\"\\n'.format(new_line=new_line.strip()))\n",
    "\n",
    "\n",
    "import os\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import csv\n",
    "import sys\n",
    "import string\n",
    "\n",
    "class LemmatiseSkills():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        self.out_df = pd.DataFrame()\n",
    "        # nltk.download('wordnet')\n",
    "\n",
    "    def lemmatise_file(self, in_df, column_name):\n",
    "        result = []\n",
    "        for index, row in in_df.iterrows():\n",
    "            line = str(row[column_name])\n",
    "            flag = 0\n",
    "            for char in line:\n",
    "                if ord(char) > 127:\n",
    "                    flag = 1\n",
    "                    break\n",
    "            if flag:\n",
    "                result.append(' ')\n",
    "                continue\n",
    "            line = line.translate(str.maketrans('-/\"()&', '      ', ''))\n",
    "            new_line = ''\n",
    "            for word in line.split(' '):\n",
    "                word = word.strip().lower()\n",
    "                lemma = self.wordnet_lemmatizer.lemmatize(word)\n",
    "                new_line += word+' ' if len(word) <= 4 else lemma+' '\n",
    "            result.append(new_line.strip())\n",
    "        self.out_df[column_name] = result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLEANING DRIVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "verified_skills_file = '/home/jasmeet16-jtg/projects/temp_scripts/skill_standardization/verified_skills.csv'\n",
    "not_verified_skills_file = '/home/jasmeet16-jtg/projects/temp_scripts/skill_standardization/input_unverified_skills.csv'\n",
    "lemmatised_not_verified_skills_file = '/home/jasmeet16-jtg/projects/temp_scripts/skill_standardization/input_unverified_skills_lemmatised.csv'\n",
    "verified_skills = pd.read_csv(verified_skills_file)\n",
    "vs1 = verified_skills[verified_skills['Status']=='Verified']\n",
    "vs2 = verified_skills[verified_skills['New Status']=='Verified']\n",
    "verified_skills = pd.concat([vs1, vs2])\n",
    "# verified_skills = pd.DataFrame(verified_skills)\n",
    "\n",
    "not_verified_skills = pd.read_csv(verified_skills_file)\n",
    "not_verified_skills = not_verified_skills[(not_verified_skills['Status'] != 'Verified')\\\n",
    "                                          & (not_verified_skills['New Status'] != 'Verified')].reset_index(drop=True)\n",
    "\n",
    "ls = LemmatiseSkills()\n",
    "ls.lemmatise_file(not_verified_skills, 'Name')\n",
    "# lemmatised_not_verified_skills = pd.read_csv(lemmatised_not_verified_skills_file, names=['lemmatised_skills'])\n",
    "\n",
    "# print(not_verified_skills.head(3), lemmatised_not_verified_skills.head(3))\n",
    "not_verified_skills['lemmatised_skill'] = ls.out_df['Name']\n",
    "# not_verified_skills = pd.concat([not_verified_skills, lemmatised_not_verified_skills], axis=1)\n",
    "display(not_verified_skills)\n",
    "not_verified_skills.to_csv('/home/jasmeet16-jtg/projects/temp_scripts/skill_standardization/temp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "system admin > data verification > verification scripts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "calyx_ds3",
   "language": "python",
   "name": "calyx_ds3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
