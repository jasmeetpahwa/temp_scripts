{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GOOGLE GEOCODING API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "import pandas as pd\n",
    "locations_file = '/home/jasmeet16-jtg/locations.csv'\n",
    "loc_df = pd.read_csv('/home/jasmeet16-jtg/locations.csv')[:1]\n",
    "api_key = 'AIzaSyDtMgHvzUPmW5uOD6CgNt8tVRVo1O0In1M'\n",
    "url = 'https://maps.googleapis.com/maps/api/geocode/json?'\n",
    "result_df = pd.DataFrame(columns=['city', 'state', 'lat', 'lng'])\n",
    "for i in range(0, len(loc_df)):\n",
    "    row = []\n",
    "    city = loc_df.loc[i]['city']\n",
    "    state = loc_df.loc[i]['city']\n",
    "    place += city + ',' + state\n",
    "    response = requests.get(url + 'address=' + place + '&key=' + api_key)\n",
    "    x = response.json()\n",
    "    lat = x['results'][0]['geometry']['location']['lat']\n",
    "    lng = x['results'][0]['geometry']['location']['lng']\n",
    "    row = [city, state, lat, lng]\n",
    "    result_df.loc[i] = row\n",
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MICROSOFT GEOCODING API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "import pandas as pd\n",
    "locations_file = '/home/jasmeet16-jtg/locations.csv'\n",
    "output_file = '/home/jasmeet16-jtg/locations_with_lng_lat.csv'\n",
    "loc_df = pd.read_csv('/home/jasmeet16-jtg/locations.csv')\n",
    "api_key = 'As1u17uf3h7eVhyvshDQEiUR24tewzQOhERscbUXJM3A4bA873A9jC73O9AjysUS'\n",
    "url = 'http://dev.virtualearth.net/REST/v1/Locations?query={}&key={}'\n",
    "\n",
    "result_df = pd.DataFrame(columns=['college', 'count', 'city', 'state', 'lat', 'lng'])\n",
    "for i in range(0, len(loc_df)):\n",
    "    row = []\n",
    "    place = ''\n",
    "    city = loc_df.loc[i]['city']\n",
    "    state = loc_df.loc[i]['state']\n",
    "    college = loc_df.loc[i]['college']\n",
    "    count = loc_df.loc[i]['hirable_students']\n",
    "    place += city + ',%20' + state\n",
    "    response = requests.get(url.format(place, api_key))\n",
    "    x = response.json()\n",
    "    box = x['resourceSets'][0]['resources'][0]['bbox']\n",
    "    lat = (box[0] + box[2])/2\n",
    "    lng = (box[1] + box[3])/2\n",
    "    row = [college, count, city, state, lat, lng]\n",
    "    result_df.loc[i] = row\n",
    "result_df.to_csv(output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLUSTERING COLLEGES BASED ON DISTANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "input_file = '/home/jasmeet16-jtg/locations_with_lng_lat.csv'\n",
    "output_file = '/home/jasmeet16-jtg/pool_colleges.csv'\n",
    "df = pd.read_csv(input_file)\n",
    "df1 = np.array(df[['lat', 'lng']])\n",
    "\n",
    "# n_clusters = 10\n",
    "# hc = AgglomerativeClustering(n_clusters=n_clusters, affinity = 'euclidean', linkage = 'ward')\n",
    "hc = AgglomerativeClustering(affinity = 'euclidean', linkage = 'ward', distance_threshold=1.4846059080767864)\n",
    "y_hc = hc.fit_predict(df1)\n",
    "\n",
    "display(y_hc)\n",
    "for i in range(0, max(y_hc)):\n",
    "    plt.scatter(df1[y_hc == i,0], df1[y_hc == i,1], s=100)\n",
    "plt.show()\n",
    "\n",
    "df = pd.concat([df, pd.DataFrame(y_hc)], axis=1)\n",
    "# df = df.sort_values(by=0)\n",
    "df.to_csv(output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLUSTERING COLLEGES BASED ON COUNT 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "input_file = '/home/jasmeet16-jtg/pool_colleges.csv'\n",
    "output_file = '/home/jasmeet16-jtg/pool_colleges_with_distance_count.csv'\n",
    "result_df = pd.DataFrame()\n",
    "df = pd.read_csv(input_file).dropna()\n",
    "req, cluster_number = 1000, 0\n",
    "result_df = pd.DataFrame(columns=['college', 'city', 'state', 'count', 'lat', 'lng', 'cluster_number'])\n",
    "for i in range(0, df['0'].nunique()):\n",
    "    temp_df = df[df['0'] == i].sort_values(by='count', ascending=False)\n",
    "    for j in range(0, len(temp_df)):\n",
    "        sum_,m = 0, 0\n",
    "        temp_df1 = pd.DataFrame(columns=['college', 'city', 'state', 'count', 'lat', 'lng', 'cluster_number'])\n",
    "        for k in range(j, len(temp_df)):\n",
    "            sum_ += temp_df.loc[temp_df.index[k]]['count']\n",
    "            temp_df1.loc[m] = temp_df.loc[temp_df.index[k]]\n",
    "            m += 1\n",
    "            if sum_ >= req or k == (len(temp_df) - 1):\n",
    "                temp_df1['cluster_number'] = cluster_number\n",
    "                cluster_number += 1\n",
    "                result_df = pd.concat([result_df, temp_df1])\n",
    "                break\n",
    "\n",
    "result_df.to_csv(output_file)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLUSTERING COLLEGES BASED ON COUNT 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 2)) while a minimum of 1 is required.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-6419de9a9a1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mdf1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'norm_count'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnorm_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mn_clusters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mkmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Unnamed: 0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'norm_count'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mdf1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'final_clusters'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'final_clusters'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/virtualenvs/calyx_ds3/lib/python3.5/site-packages/sklearn/cluster/k_means_.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    967\u001b[0m                 \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy_x\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 969\u001b[0;31m                 return_n_iter=True)\n\u001b[0m\u001b[1;32m    970\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/virtualenvs/calyx_ds3/lib/python3.5/site-packages/sklearn/cluster/k_means_.py\u001b[0m in \u001b[0;36mk_means\u001b[0;34m(X, n_clusters, sample_weight, init, precompute_distances, n_init, max_iter, verbose, tol, random_state, copy_x, n_jobs, algorithm, return_n_iter)\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0morder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"C\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcopy_x\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     X = check_array(X, accept_sparse='csr', dtype=[np.float64, np.float32],\n\u001b[0;32m--> 309\u001b[0;31m                     order=order, copy=copy_x)\n\u001b[0m\u001b[1;32m    310\u001b[0m     \u001b[0;31m# verify that the number of samples given is larger than k\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mn_clusters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/virtualenvs/calyx_ds3/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    548\u001b[0m                              \u001b[0;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m                              % (n_samples, array.shape, ensure_min_samples,\n\u001b[0;32m--> 550\u001b[0;31m                                 context))\n\u001b[0m\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_features\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 2)) while a minimum of 1 is required."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "input_file = '/home/jasmeet16-jtg/pool_colleges.csv'\n",
    "df = pd.read_csv(input_file).dropna()\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "for i in range(df['0'].nunique()):\n",
    "    df1 = df[df['0']==i]\n",
    "    max_ = df1['Unnamed: 0'].max()\n",
    "    max_count = df1['count'].max()\n",
    "    norm_count = []\n",
    "    for row in df1.iterrows():\n",
    "        norm_count.append((row[1][3]/max_count)*max_)\n",
    "    df1['norm_count'] = norm_count\n",
    "    n_clusters = 4 if len(df1) >= 4 else len(df1)\n",
    "    kmeans = KMeans(n_clusters=n_clusters).fit(df1[['Unnamed: 0', 'norm_count']])\n",
    "    df1['final_clusters'] = list(kmeans.labels_)\n",
    "    df1.sort_values(by='final_clusters')\n",
    "    result_df = pd.concat([result_df, df1])\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.5"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "calyx_ds3",
   "language": "python",
   "name": "calyx_ds3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
